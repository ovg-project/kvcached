#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„æ‰¹é‡benchmarkè„šæœ¬
ç›´æ¥å†…åµŒæ‰€æœ‰åŠŸèƒ½ï¼Œä¸è°ƒç”¨å…¶ä»–è„šæœ¬ï¼Œæ”¯æŒå®æ—¶è¾“å‡º
"""

import argparse
import json
import subprocess
import time
import sys
import yaml
import os
import threading
import requests
from datetime import datetime
from pathlib import Path
import psutil

def wait_for_health_check(port, timeout=120):
    """ç­‰å¾…å®ä¾‹å¥åº·æ£€æŸ¥é€šè¿‡"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"http://localhost:{port}/get_model_info", timeout=5)
            if response.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(3)
    return False


def load_experiments_config(config_file):
    """åŠ è½½å®éªŒé…ç½®æ–‡ä»¶"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_sglang_config(config_file):
    """åŠ è½½SGLangé…ç½®æ–‡ä»¶"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def kill_port_processes(ports):
    """æ€æ­»å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    print("ğŸ”ª æ¸…ç†ç«¯å£å ç”¨çš„è¿›ç¨‹...")
    
    for port in ports:
        try:
            result = subprocess.run(
                ["lsof", "-ti", f":{port}"], 
                capture_output=True, text=True
            )
            if result.returncode == 0 and result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    if pid:
                        print(f"  æ€æ­»ç«¯å£ {port} çš„è¿›ç¨‹ PID: {pid}")
                        subprocess.run(["kill", "-9", pid])
            else:
                print(f"  ç«¯å£ {port} æœªè¢«å ç”¨")
        except Exception as e:
            print(f"  æ¸…ç†ç«¯å£ {port} æ—¶å‡ºé”™: {e}")

def start_model_instances(config, log_dir, env_vars):
    """å¯åŠ¨æ¨¡å‹å®ä¾‹"""
    print("ğŸš€ å¯åŠ¨æ¨¡å‹å®ä¾‹...")
    
    instances = config.get("instances", [])
    processes = []
    
    for i, instance in enumerate(instances):
        instance_name = instance.get("name", "unknown")
        model_name = instance.get("model", "")
        engine_args = instance.get("engine_args", [])
        
        print(f"  å¯åŠ¨å®ä¾‹: {instance_name}")
        
        # æ„å»ºå¯åŠ¨å‘½ä»¤
        cmd = ["python3", "-m", "sglang.launch_server", "--model-path", model_name]
        cmd.extend(engine_args)
        
        # å‡†å¤‡ç¯å¢ƒå˜é‡
        instance_env = env_vars.copy()
        
        # æ·»åŠ å®ä¾‹ç‰¹å®šçš„ç¯å¢ƒå˜é‡
        for env_item in instance.get("kvcached_env", []):
            if "=" in env_item:
                key, value = env_item.split("=", 1)
                instance_env[key] = value
        
        for env_item in instance.get("engine_env", []):
            if "=" in env_item:
                key, value = env_item.split("=", 1)
                instance_env[key] = value
        
        # å¯åŠ¨è¿›ç¨‹
        log_file = log_dir / f"{instance_name}_server.log"
        with open(log_file, 'w') as f:
            f.write(f"=== {instance_name} æœåŠ¡å™¨æ—¥å¿— ===\n")
            f.write(f"å¯åŠ¨æ—¶é—´: {datetime.now()}\n")
            f.write(f"å‘½ä»¤: {' '.join(cmd)}\n")
            f.write("=" * 50 + "\n\n")
            f.flush()
            
            process = subprocess.Popen(
                cmd,
                stdout=f,
                stderr=subprocess.STDOUT,
                cwd="/workspace/kvcached/controller",
                env=instance_env,
                text=True
            )
            
        processes.append((instance_name, process, log_file))
        print(f"    PID: {process.pid}")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå®ä¾‹ï¼Œç­‰å¾…å…¶å¥åº·æ£€æŸ¥é€šè¿‡å†å¯åŠ¨ä¸‹ä¸€ä¸ª
        if i == 0 and len(instances) > 1:
            port = None
            for arg in engine_args:
                if arg.startswith("--port="):
                    port = int(arg.split("=")[1])
                    break
            
            if port:
                print(f"    ç­‰å¾…å®ä¾‹ {instance_name} (ç«¯å£:{port}) å¥åº·æ£€æŸ¥...")
                if wait_for_health_check(port, timeout=120):
                    print(f"    âœ… å®ä¾‹ {instance_name} å·²å°±ç»ªï¼Œç»§ç»­å¯åŠ¨ä¸‹ä¸€ä¸ªå®ä¾‹")
                else:
                    print(f"    âŒ å®ä¾‹ {instance_name} å¥åº·æ£€æŸ¥å¤±è´¥")
                    break
        elif i > 0:
            time.sleep(5)  # éç¬¬ä¸€ä¸ªå®ä¾‹é—´çš„å°å»¶è¿Ÿ
    
    return processes

def start_router(config, log_dir, env_vars):
    """å¯åŠ¨è·¯ç”±å™¨"""
    router_config = config.get("router", {})
    if not router_config.get("enable_router", True):
        return None
    
    print("ğŸš€ å¯åŠ¨è·¯ç”±å™¨...")
    
    config_file = "temp_config.yaml"
    with open(config_file, 'w') as f:
        yaml.dump(config, f)
    
    cmd = ["python3", "frontend.py", "--config", config_file]
    
    log_file = log_dir / "router.log"
    with open(log_file, 'w') as f:
        f.write("=== è·¯ç”±å™¨æ—¥å¿— ===\n")
        f.write(f"å¯åŠ¨æ—¶é—´: {datetime.now()}\n")
        f.write(f"å‘½ä»¤: {' '.join(cmd)}\n")
        f.write("=" * 50 + "\n\n")
        f.flush()
        
        process = subprocess.Popen(
            cmd,
            stdout=f,
            stderr=subprocess.STDOUT,
            cwd="/workspace/kvcached/controller",
            env=env_vars,
            text=True
        )
    
    print(f"  è·¯ç”±å™¨PID: {process.pid}")
    return process, log_file

def wait_for_service_health(port, instance_name, max_wait=60):
    """ç­‰å¾…æœåŠ¡å¥åº·"""
    print(f"ğŸ¥ ç­‰å¾…å®ä¾‹ {instance_name} å¥åº· (ç«¯å£ {port})...")
    
    for i in range(max_wait):
        try:
            result = subprocess.run([
                "curl", "-s", f"http://localhost:{port}/get_model_info"
            ], capture_output=True, text=True, timeout=3)
            
            if result.returncode == 0 and "model_path" in result.stdout:
                print(f"  âœ… å®ä¾‹ {instance_name} å¥åº· (è€—æ—¶ {i+1}s)")
                return True
        except Exception:
            pass
        
        if (i + 1) % 10 == 0:
            print(f"  â³ ç­‰å¾…ä¸­... ({i+1}s/{max_wait}s)")
        time.sleep(1)
    
    print(f"  âŒ å®ä¾‹ {instance_name} å¥åº·æ£€æŸ¥è¶…æ—¶")
    return False

def run_benchmark_for_instance(instance_name, model_name, port, log_file, 
                               request_rate, num_prompts, max_concurrency=None):
    """ä¸ºå•ä¸ªå®ä¾‹è¿è¡Œbenchmark"""
    print(f"\nğŸ§ª å¼€å§‹å®ä¾‹ {instance_name} çš„benchmark...")
    print(f"  ç«¯å£: {port}, é€Ÿç‡: {request_rate} req/s, è¯·æ±‚æ•°: {num_prompts}")
    
    # å‡†å¤‡ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    venv_bin = "/workspace/kvcached/engine_integration/sglang-v0.4.9/.venv/bin"
    env['PATH'] = f"{venv_bin}:{env['PATH']}"
    env['VIRTUAL_ENV'] = "/workspace/kvcached/engine_integration/sglang-v0.4.9/.venv"
    
    # æ„å»ºbenchmarkå‘½ä»¤
    cmd = [
        "python3", "-m", "sglang.bench_serving",
        "--backend", "sglang-oai",
        "--model", model_name,
        "--dataset-name", "sharegpt",
        "--dataset-path", "/workspace/kvcached/engine_integration/benchmark/ShareGPT_V3_unfiltered_cleaned_split.json",
        "--request-rate", str(request_rate),
        "--num-prompts", str(num_prompts),
        "--port", str(port),
        "--host", "localhost"
    ]
    
    if max_concurrency:
        cmd.extend(["--max-concurrency", str(max_concurrency)])
    
    print(f"  ğŸ’» å‘½ä»¤: {' '.join(cmd)}")
    
    # è¿è¡Œbenchmarkï¼Œå®æ—¶æ˜¾ç¤ºè¾“å‡º
    with open(log_file, 'w') as f:
        f.write(f"=== {instance_name} Benchmark æ—¥å¿— ===\n")
        f.write(f"å®ä¾‹: {instance_name}\n")
        f.write(f"ç«¯å£: {port}\n")
        f.write(f"è¯·æ±‚é€Ÿç‡: {request_rate} req/s\n")
        f.write(f"æ€»è¯·æ±‚æ•°: {num_prompts}\n")
        f.write(f"å‘½ä»¤: {' '.join(cmd)}\n")
        f.write("=" * 50 + "\n\n")
        f.flush()
        
        # å¯åŠ¨è¿›ç¨‹ï¼Œå¹¶å®æ—¶æ˜¾ç¤ºè¾“å‡º
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            cwd="/workspace/kvcached/controller",
            env=env,
            text=True,
            bufsize=1
        )
        
        # å®æ—¶è¯»å–å’Œæ˜¾ç¤ºè¾“å‡º
        while True:
            line = process.stdout.readline()
            if line:
                # åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ˜¾ç¤ºåˆ°æ§åˆ¶å°
                f.write(line)
                f.flush()
                print(f"[{instance_name}] {line.rstrip()}")
            elif process.poll() is not None:
                break
        
        # è·å–å‰©ä½™è¾“å‡º
        remaining = process.stdout.read()
        if remaining:
            f.write(remaining)
            print(f"[{instance_name}] {remaining.rstrip()}")
        
        return_code = process.wait()
        
        f.write(f"\n=== Benchmark å®Œæˆ ===\n")
        f.write(f"è¿”å›ä»£ç : {return_code}\n")
    
    success = return_code == 0
    if success:
        print(f"âœ… å®ä¾‹ {instance_name} benchmarkå®Œæˆ")
    else:
        print(f"âŒ å®ä¾‹ {instance_name} benchmarkå¤±è´¥ (ä»£ç : {return_code})")
    
    return success

def run_parallel_benchmarks(instances, request_rate, num_prompts, log_dir, max_concurrency=None):
    """å¹¶è¡Œè¿è¡Œå¤šä¸ªå®ä¾‹çš„benchmark"""
    print(f"\nğŸ”„ å¼€å§‹å¹¶è¡Œbenchmarkæµ‹è¯•...")
    print(f"ğŸ“Š å°†åŒæ—¶æµ‹è¯• {len(instances)} ä¸ªå®ä¾‹")
    
    threads = []
    results = {}
    
    def benchmark_worker(instance_name, model_name, port, log_file):
        success = run_benchmark_for_instance(
            instance_name, model_name, port, log_file,
            request_rate, num_prompts, max_concurrency
        )
        results[instance_name] = success
    
    # å¯åŠ¨æ‰€æœ‰benchmarkçº¿ç¨‹
    for instance in instances:
        instance_name = instance.get("name", "unknown")
        model_name = instance.get("model", "")
        
        # ä»engine_argsä¸­æå–ç«¯å£
        port = None
        for arg in instance.get("engine_args", []):
            if arg.startswith("--port="):
                port = int(arg.split("=")[1])
                break
        
        if not port:
            print(f"âŒ æ— æ³•è·å–å®ä¾‹ {instance_name} çš„ç«¯å£")
            results[instance_name] = False
            continue
        
        log_file = log_dir / f"{instance_name}_benchmark.log"
        
        thread = threading.Thread(
            target=benchmark_worker,
            args=(instance_name, model_name, port, log_file)
        )
        thread.start()
        threads.append(thread)
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    return results

def kill_gpu_processes_by_nvidia_smi():
    """é€šè¿‡nvidia-smiæŸ¥è¯¢å¹¶ç»ˆæ­¢ä½¿ç”¨GPUçš„è¿›ç¨‹"""
    print("ğŸ” é€šè¿‡ nvidia-smi æŸ¥è¯¢ä½¿ç”¨ GPU çš„è¿›ç¨‹...")
    try:
        # è·å– pid, process_name, used_gpu_memory ç­‰ä¿¡æ¯
        output = subprocess.check_output([
            "nvidia-smi", 
            "--query-compute-apps=pid,process_name,used_gpu_memory", 
            "--format=csv,noheader"
        ]).decode("utf-8").strip()
    except subprocess.CalledProcessError as e:
        print(f"è°ƒç”¨ nvidia-smi å‡ºé”™: {e}")
        return
    except FileNotFoundError:
        print("nvidia-smi å‘½ä»¤æœªæ‰¾åˆ°ï¼Œè·³è¿‡GPUè¿›ç¨‹æ¸…ç†")
        return

    lines = output.split("\n")
    if not lines or not lines[0]:
        print("âœ… å½“å‰æ²¡æœ‰è¿›ç¨‹åœ¨ä½¿ç”¨ GPU")
        return

    # å¾ªç¯è§£æå¹¶ kill
    killed_count = 0
    for line in lines:
        parts = line.strip().split(",")
        if len(parts) < 3:
            continue
        pid_str = parts[0].strip()
        proc_name = parts[1].strip()
        mem_usage = parts[2].strip()
        if not pid_str.isdigit():
            continue

        pid = int(pid_str)
        print(f"ğŸ¯ æ‰¾åˆ° GPU å ç”¨è¿›ç¨‹: PID={pid}, Name={proc_name}, Memory={mem_usage} MiB")
        try:
            p = psutil.Process(pid)
            # å…ˆå°è¯•æ­£å¸¸ç»ˆæ­¢
            p.terminate()
            try:
                p.wait(timeout=5)  # ç­‰å¾… 5 ç§’
                print(f"âœ… å·²æ­£å¸¸ç»ˆæ­¢è¿›ç¨‹ {pid}")
                killed_count += 1
            except psutil.TimeoutExpired:
                print(f"âš¡ è¿›ç¨‹ {pid} æœªåœ¨è§„å®šæ—¶é—´å†…é€€å‡ºï¼Œå°è¯•å¼ºåˆ¶ kill...")
                p.kill()
                print(f"ğŸ’€ å·²å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ {pid}")
                killed_count += 1
        except psutil.NoSuchProcess:
            print(f"âš ï¸  è¿›ç¨‹ {pid} å·²ä¸å­˜åœ¨")
        except psutil.AccessDenied:
            print(f"ğŸš« æ— æƒé™ç»ˆæ­¢è¿›ç¨‹ {pid}")
        except Exception as e:
            print(f"âŒ ç»ˆæ­¢è¿›ç¨‹ {pid} å‡ºé”™: {e}")

    if killed_count > 0:
        print(f"ğŸ§¹ GPU è¿›ç¨‹æ¸…ç†å®Œæ¯•ï¼Œå…±ç»ˆæ­¢äº† {killed_count} ä¸ªè¿›ç¨‹")
    else:
        print("âœ… GPU è¿›ç¨‹æ¸…ç†å®Œæ¯•")


def cleanup_processes(processes):
    """æ¸…ç†è¿›ç¨‹"""
    print("\nğŸ§¹ æ¸…ç†æ‰€æœ‰è¿›ç¨‹...")
    
    if isinstance(processes, list):
        for name, process, log_file in processes:
            if process and process.poll() is None:
                print(f"  åœæ­¢ {name} (PID: {process.pid})")
                process.terminate()
                try:
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
    elif processes:
        process, log_file = processes
        if process and process.poll() is None:
            print(f"  åœæ­¢è·¯ç”±å™¨ (PID: {process.pid})")
            process.terminate()
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()

def run_single_experiment(experiment, global_settings, base_timestamp):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    exp_name = experiment["name"]
    config_file = experiment["config_file"]
    request_rate = experiment["request_rate"]
    num_prompts = experiment["num_prompts"]
    max_concurrency = experiment.get("max_concurrency")
    kvcached_enabled = experiment.get("kvcached_enabled", True)
    
    print("=" * 80)
    print(f"ğŸ§ª å¼€å§‹å®éªŒ: {exp_name}")
    print(f"ğŸ“‹ æè¿°: {experiment['description']}")
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {config_file}")
    print(f"ğŸ“Š è¯·æ±‚é€Ÿç‡: {request_rate} req/s")
    print(f"ğŸ“ æ€»è¯·æ±‚æ•°: {num_prompts}")
    print(f"ğŸ”§ KVCached: {'å¯ç”¨' if kvcached_enabled else 'ç¦ç”¨'}")
    print("=" * 80)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    task_name = f"{base_timestamp}_{exp_name}"
    log_dir = Path(f"/workspace/kvcached/yangmin_logs/{task_name}")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    start_time = time.time()
    model_processes = []
    router_process = None
    
    try:
        # åŠ è½½é…ç½®
        config = load_sglang_config(config_file)
        instances = config.get("instances", [])
        
        # å‡†å¤‡ç¯å¢ƒå˜é‡
        env_vars = os.environ.copy()
        venv_bin = "/workspace/kvcached/engine_integration/sglang-v0.4.9/.venv/bin"
        env_vars['PATH'] = f"{venv_bin}:{env_vars['PATH']}"
        env_vars['VIRTUAL_ENV'] = "/workspace/kvcached/engine_integration/sglang-v0.4.9/.venv"
        env_vars['CUDA_VISIBLE_DEVICES'] = "0"
        
        # 1. æ¸…ç†ç«¯å£
        ports_to_clear = [8080]  # è·¯ç”±å™¨ç«¯å£
        for instance in instances:
            for arg in instance.get("engine_args", []):
                if arg.startswith("--port="):
                    ports_to_clear.append(int(arg.split("=")[1]))
        
        kill_port_processes(ports_to_clear)
        
        # 2. å¯åŠ¨æ¨¡å‹å®ä¾‹
        model_processes = start_model_instances(config, log_dir, env_vars)
        
        # 3. ç­‰å¾…å®ä¾‹å¥åº·
        all_healthy = True
        for instance in instances:
            instance_name = instance.get("name", "unknown")
            port = None
            for arg in instance.get("engine_args", []):
                if arg.startswith("--port="):
                    port = int(arg.split("=")[1])
                    break
            
            if port and not wait_for_service_health(port, instance_name):
                all_healthy = False
                break
        
        if not all_healthy:
            print("âŒ å®ä¾‹å¥åº·æ£€æŸ¥å¤±è´¥")
            return {"name": exp_name, "success": False, "error": "health_check_failed"}
        
        # 4. è¿è¡Œå¹¶è¡Œbenchmark
        benchmark_results = run_parallel_benchmarks(
            instances, request_rate, num_prompts, log_dir, max_concurrency
        )
        
        # è®¡ç®—ç»“æœ
        successful_benchmarks = sum(1 for success in benchmark_results.values() if success)
        total_benchmarks = len(benchmark_results)
        
        end_time = time.time()
        duration_minutes = (end_time - start_time) / 60
        
        overall_success = successful_benchmarks == total_benchmarks
        
        result = {
            "name": exp_name,
            "success": overall_success,
            "duration_minutes": round(duration_minutes, 2),
            "task_name": task_name,
            "benchmark_results": benchmark_results,
            "successful_benchmarks": successful_benchmarks,
            "total_benchmarks": total_benchmarks,
            "start_time": datetime.fromtimestamp(start_time).isoformat(),
            "end_time": datetime.fromtimestamp(end_time).isoformat()
        }
        
        if overall_success:
            print(f"\nâœ… å®éªŒ {exp_name} å®Œæˆ ({successful_benchmarks}/{total_benchmarks} æˆåŠŸ, è€—æ—¶: {duration_minutes:.1f} åˆ†é’Ÿ)")
        else:
            print(f"\nâŒ å®éªŒ {exp_name} éƒ¨åˆ†å¤±è´¥ ({successful_benchmarks}/{total_benchmarks} æˆåŠŸ, è€—æ—¶: {duration_minutes:.1f} åˆ†é’Ÿ)")
        
        return result
        
    except Exception as e:
        end_time = time.time()
        duration_minutes = (end_time - start_time) / 60
        print(f"\nğŸ’¥ å®éªŒ {exp_name} å‡ºé”™: {e}")
        
        return {
            "name": exp_name,
            "success": False,
            "duration_minutes": round(duration_minutes, 2),
            "task_name": task_name,
            "error": str(e),
            "start_time": datetime.fromtimestamp(start_time).isoformat(),
            "end_time": datetime.fromtimestamp(end_time).isoformat()
        }
    
    finally:
        # æ¸…ç†è¿›ç¨‹
        cleanup_processes(model_processes)
        if router_process:
            cleanup_processes(router_process)
        
        # æ¸…ç†GPUè¿›ç¨‹
        kill_gpu_processes_by_nvidia_smi()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿èµ„æºè¢«é‡Šæ”¾
        time.sleep(3)

def main():
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹æ‰¹é‡benchmarkå®éªŒ")
    parser.add_argument("--config", default="benchmark_experiments.json",
                        help="å®éªŒé…ç½®æ–‡ä»¶")
    parser.add_argument("--start-from", type=int, default=0,
                        help="ä»ç¬¬å‡ ä¸ªå®éªŒå¼€å§‹")
    parser.add_argument("--max-experiments", type=int,
                        help="æœ€å¤šè¿è¡Œå‡ ä¸ªå®éªŒ")
    parser.add_argument("--dry-run", action="store_true",
                        help="ä»…æ˜¾ç¤ºå®éªŒåˆ—è¡¨")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config_file = Path(args.config)
    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return 1
    
    experiments_config = load_experiments_config(config_file)
    experiments = experiments_config["experiments"]
    global_settings = experiments_config.get("global_settings", {})
    
    # å¤„ç†å®éªŒèŒƒå›´
    start_idx = args.start_from
    if args.max_experiments:
        end_idx = min(start_idx + args.max_experiments, len(experiments))
    else:
        end_idx = len(experiments)
    
    selected_experiments = experiments[start_idx:end_idx]
    
    # åˆ›å»ºæ‰¹æ¬¡æ—¶é—´æˆ³
    base_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("ğŸš€ ç‹¬ç«‹æ‰¹é‡Benchmarkå®éªŒå¼€å§‹")
    print(f"ğŸ“… æ‰¹æ¬¡æ—¶é—´æˆ³: {base_timestamp}")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_file}")
    print(f"ğŸ§ª æ€»å®éªŒæ•°: {len(experiments)}")
    print(f"ğŸ“Š å°†æ‰§è¡Œ: ç¬¬{start_idx}åˆ°{end_idx-1}ä¸ªå®éªŒ (å…±{len(selected_experiments)}ä¸ª)")
    print("=" * 80)
    
    if args.dry_run:
        print("ğŸ” Dry-runæ¨¡å¼ï¼Œå°†è¦æ‰§è¡Œçš„å®éªŒ:")
        for i, exp in enumerate(selected_experiments, start_idx):
            print(f"  {i}: {exp['name']} - {exp['description']}")
        return 0
    
    # æ‰§è¡Œå®éªŒ
    results = []
    total_start_time = time.time()
    
    for i, experiment in enumerate(selected_experiments, start_idx):
        print(f"\nğŸ“Š è¿›åº¦: {i-start_idx+1}/{len(selected_experiments)}")
        
        result = run_single_experiment(experiment, global_settings, base_timestamp)
        results.append(result)
        
        # å®éªŒé—´æ¸…ç†
        if i < len(selected_experiments) - 1:
            print("ğŸ§¹ å®éªŒé—´æ¸…ç†...")
            time.sleep(5)
    
    total_end_time = time.time()
    total_duration = (total_end_time - total_start_time) / 60
    
    # ç”Ÿæˆæ±‡æ€»
    successful = sum(1 for r in results if r["success"])
    failed = len(results) - successful
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰¹é‡å®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æ€»ç»“æœ: {successful}/{len(results)} æˆåŠŸ, {failed}/{len(results)} å¤±è´¥")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f} åˆ†é’Ÿ")
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    report_dir = Path(f"/workspace/kvcached/yangmin_logs/{base_timestamp}_batch_summary")
    report_dir.mkdir(parents=True, exist_ok=True)
    
    with open(report_dir / "batch_report.json", 'w', encoding='utf-8') as f:
        json.dump({
            "batch_timestamp": base_timestamp,
            "total_experiments": len(results),
            "successful_experiments": successful,
            "failed_experiments": failed,
            "total_duration_minutes": total_duration,
            "experiments": results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_dir}")
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main()) 