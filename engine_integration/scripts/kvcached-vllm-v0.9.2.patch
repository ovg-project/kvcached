diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py
index d21f94727..f53b52a9e 100644
--- a/vllm/v1/core/block_pool.py
+++ b/vllm/v1/core/block_pool.py
@@ -347,3 +347,112 @@ class BlockPool:
         events = self.kv_event_queue
         self.kv_event_queue = []
         return events
+
+
+class ElasticBlockPool(BlockPool):
+    """ElasticBlockPool that manages KVCacheBlocks.
+    It provides same interface as BlockPool, but it leverages kvcached for
+    elastic KV cachememory management.
+    """
+
+    def __init__(self,
+                 num_gpu_blocks: int,
+                 block_size: int,
+                 cell_size: int,
+                 num_layers: int,
+                 enable_caching: bool,
+                 enable_kv_cache_events: bool = False):
+        assert isinstance(num_gpu_blocks, int) and num_gpu_blocks > 0
+        assert not enable_caching, (
+            "Caching is not supported in ElasticBlockPool")
+        assert not enable_kv_cache_events, (
+            "KV cache events are not supported in ElasticBlockPool")
+
+        self.num_gpu_blocks = num_gpu_blocks
+        self.enable_kv_cache_events = enable_kv_cache_events
+        self.kv_event_queue: list[KVCacheEvent] = []
+
+        from kvcached.integration.vllm.interfaces import (  # noqa: E501
+            get_kv_cache_manager)
+        self.kv_cache_manager = get_kv_cache_manager(num_gpu_blocks,
+                                                     block_size, cell_size,
+                                                     num_layers)
+
+        self.null_block = None  # type: ignore
+
+    def get_cached_block(
+            self, block_hash: BlockHash,
+            kv_cache_group_ids: list[int]) -> Optional[list[KVCacheBlock]]:
+        return None
+
+    def cache_full_blocks(
+        self,
+        request: Request,
+        blocks: list[KVCacheBlock],
+        block_hashes: list[BlockHash],
+        num_cached_blocks: int,
+        num_full_blocks: int,
+        block_size: int,
+        kv_cache_group_id: int,
+        hash_fn: Callable,
+    ) -> None:
+        raise NotImplementedError(
+            "Caching is not supported in ElasticBlockPool")
+
+    def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:
+        """Get new blocks from the free block pool.
+
+        Note that we do not check block cache in this function.
+
+        Args:
+            num_blocks: The number of blocks to allocate.
+
+        Returns:
+            A list of new block.
+        """
+        if num_blocks > self.get_num_free_blocks():
+            raise ValueError(
+                f"Cannot get {num_blocks} free blocks from the pool")
+
+        block_ids = self.kv_cache_manager.alloc(num_blocks)
+        assert block_ids is not None and len(block_ids) == num_blocks
+
+        return [KVCacheBlock(bid) for bid in block_ids]
+
+    def touch(self, blocks: tuple[list[KVCacheBlock], ...]) -> None:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -> None:
+        """Free a list of blocks. The blocks should be ordered by their
+        eviction priority, where the first block will be evicted first.
+
+        Args:
+            ordered_blocks: A list of blocks to free ordered by their eviction
+                priority.
+        """
+        block_ids = [block.block_id for block in ordered_blocks]
+        if len(block_ids) > 0:
+            self.kv_cache_manager.free(block_ids)
+
+    def reset_prefix_cache(self) -> bool:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def get_num_free_blocks(self) -> int:
+        """Get the number of free blocks in the pool.
+
+        Returns:
+            The number of free blocks.
+        """
+        return self.kv_cache_manager.available_size()
+
+    def get_usage(self) -> float:
+        """Get the KV cache usage.
+
+        Returns:
+            The KV cache usage (between 0.0 and 1.0).
+        """
+        return 1.0 - (self.get_num_free_blocks() / self.num_gpu_blocks)
+
+    def take_events(self) -> list[KVCacheEvent]:
+        """ElasticBlockPool does not generate events; always return empty."""
+        return []
diff --git a/vllm/v1/core/kv_cache_coordinator.py b/vllm/v1/core/kv_cache_coordinator.py
index 38de00625..e4435cb04 100644
--- a/vllm/v1/core/kv_cache_coordinator.py
+++ b/vllm/v1/core/kv_cache_coordinator.py
@@ -29,8 +29,38 @@ class KVCacheCoordinator(ABC):
         self.max_model_len = max_model_len
         self.enable_caching = enable_caching
 
-        self.block_pool = BlockPool(kv_cache_config.num_blocks, enable_caching,
-                                    enable_kv_cache_events)
+        import os  # noqa: E501
+
+        self.enable_kvcached = os.getenv("ENABLE_KVCACHED",
+                                         "false").lower() == "true"
+
+        self.block_pool: BlockPool
+        if self.enable_kvcached:
+            if self.enable_caching:
+                raise ValueError("Caching is not supported for kvcached")
+
+            if len(self.kv_cache_config.kv_cache_groups) != 1:
+                raise ValueError(
+                    "Only one kv cache group is supported for kvcached")
+
+            kv_cache_group = self.kv_cache_config.kv_cache_groups[0]
+            block_size = kv_cache_group.kv_cache_spec.block_size
+            num_gpu_blocks = self.kv_cache_config.num_blocks
+
+            # cell_size is the size (in bytes) of the per-token K/V cache
+            cell_size = (kv_cache_group.kv_cache_spec.page_size_bytes //
+                         block_size // 2)
+
+            from vllm.v1.core.block_pool import ElasticBlockPool  # noqa: E501
+            self.block_pool = ElasticBlockPool(
+                num_gpu_blocks,
+                block_size,
+                cell_size=cell_size,
+                num_layers=len(self.kv_cache_config.kv_cache_tensors),
+                enable_caching=enable_caching)
+        else:
+            self.block_pool = BlockPool(kv_cache_config.num_blocks,
+                                        enable_caching, enable_kv_cache_events)
 
         # Needs special handling for find_longest_cache_hit if eagle is enabled
         self.use_eagle = use_eagle
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index e2fdf6f8a..d3bb46d5d 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -86,6 +86,16 @@ class EngineCore:
         self.collective_rpc("initialize_cache",
                             args=(num_gpu_blocks, num_cpu_blocks))
 
+        enable_kvcached = os.getenv("ENABLE_KVCACHED",
+                                    "false").lower() == "true"
+        if enable_kvcached:
+            from kvcached.integration.vllm.interfaces import init_kvcached
+            init_kvcached(
+                tp_rank=0,
+                tp_size=vllm_config.parallel_config.tensor_parallel_size,
+                is_worker=False,
+            )
+
         self.structured_output_manager = StructuredOutputManager(vllm_config)
 
         # Setup scheduler.
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5a26e88db..f440124d5 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -51,7 +51,8 @@ from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
                                               CommonAttentionMetadata)
 from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
 from vllm.v1.kv_cache_interface import (AttentionSpec, FullAttentionSpec,
-                                        KVCacheConfig, KVCacheSpec, MambaSpec,
+                                        KVCacheConfig, KVCacheSpec,
+                                        KVCacheTensor, MambaSpec,
                                         SlidingWindowSpec)
 from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsTensors,
                              ModelRunnerOutput)
@@ -311,6 +312,24 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                                         pin_memory=self.pin_memory)
         self.seq_lens_np = self.seq_lens_cpu.numpy()
 
+        import os
+        self.enable_kvcached = os.getenv("ENABLE_KVCACHED",
+                                         "false").lower() == "true"
+
+        if self.enable_kvcached:
+            import kvcached.integration.vllm.interfaces as kvcached_interfaces
+            self.kvcached_interfaces = kvcached_interfaces
+            # Get tensor parallel rank and size from vLLM's parallel state
+            from vllm.distributed.parallel_state import (
+                get_tensor_model_parallel_rank,
+                get_tensor_model_parallel_world_size)
+            tp_rank = get_tensor_model_parallel_rank()
+            tp_size = get_tensor_model_parallel_world_size()
+            kvcached_interfaces.init_kvcached(tp_rank=tp_rank,
+                                              tp_size=tp_size,
+                                              is_worker=True,
+                                              device=str(self.device))
+
         # Layer pairings for cross-layer KV sharing.
         # If an Attention layer `layer_name` is in the keys of this dict, it
         # means this layer will perform attention using the keys and values
@@ -2564,20 +2583,29 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             Dict[str, torch.Tensor]: A map between layer names to their
             corresponding memory buffer for KV cache.
         """
-        # Initialize the memory buffer for KV cache
-        kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
-        # Change the memory buffer to the desired shape
-        kv_caches = self._reshape_kv_cache_tensors(kv_cache_config,
-                                                   kv_cache_raw_tensors)
-
-        # Setup `kv_cache_config` and `kv_caches` for models
-        # with cross-layer KV sharing
-        if self.shared_kv_cache_layers:
-            initialize_kv_cache_for_kv_sharing(
-                self.shared_kv_cache_layers,
-                kv_cache_config.kv_cache_groups,
-                kv_caches,
-            )
+
+        if self.enable_kvcached:
+            kv_caches = self._allocate_kv_cache_from_kvcached(kv_cache_config)
+            if self.shared_kv_cache_layers:
+                raise NotImplementedError(
+                    "Cross layer KV sharing is not supported with kvcached yet."
+                )
+        else:
+            # Initialize the memory buffer for KV cache
+            kv_cache_raw_tensors = self._allocate_kv_cache_tensors(
+                kv_cache_config)
+            # Change the memory buffer to the desired shape
+            kv_caches = self._reshape_kv_cache_tensors(kv_cache_config,
+                                                       kv_cache_raw_tensors)
+
+            # Setup `kv_cache_config` and `kv_caches` for models
+            # with cross-layer KV sharing
+            if self.shared_kv_cache_layers:
+                initialize_kv_cache_for_kv_sharing(
+                    self.shared_kv_cache_layers,
+                    kv_cache_config.kv_cache_groups,
+                    kv_caches,
+                )
 
         bind_kv_cache(kv_caches,
                       self.compilation_config.static_forward_context,
@@ -2737,3 +2765,76 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 "the mamba page size")
 
         return attn_page_size
+
+    def _allocate_kv_cache_from_kvcached(
+            self, kv_cache_config: KVCacheConfig) -> dict[str, torch.Tensor]:
+        """Allocate raw KV-cache buffers via the *kvcached* backend.
+
+        Returns a flat list whose order corresponds to
+        `kv_cache_group.layer_names`.
+        """
+
+        if len(kv_cache_config.kv_cache_groups) > 1:
+            raise NotImplementedError(
+                "Hybrid models with more than one KV cache type are not "
+                "supported yet.")
+
+        kv_cache_group = kv_cache_config.kv_cache_groups[0]
+        kv_cache_spec = kv_cache_group.kv_cache_spec
+        if not isinstance(kv_cache_spec, FullAttentionSpec):
+            raise ValueError("kvcached only supports FullAttentionSpec layers")
+
+        # Build a lookup: layer_name -> KVCacheTensor config for quick access.
+        layer_to_tensor_cfg: dict[str, KVCacheTensor] = {}
+        for tensor_cfg in kv_cache_config.kv_cache_tensors:
+            for ln in tensor_cfg.shared_by:
+                layer_to_tensor_cfg[ln] = tensor_cfg
+
+        # Validate sizes and derive representative num_blocks.
+        for layer_name in kv_cache_group.layer_names:
+            tensor_cfg = layer_to_tensor_cfg[layer_name]
+            assert (tensor_cfg.size % kv_cache_spec.page_size_bytes == 0), (
+                f"Tensor size for layer {layer_name} ({tensor_cfg.size}) "
+                "is not a multiple of page size "
+                f"{kv_cache_spec.page_size_bytes}.")
+            num_blocks = tensor_cfg.size // kv_cache_spec.page_size_bytes
+            assert num_blocks >= kv_cache_config.num_blocks, (
+                "Number of blocks derived from tensor size is smaller than "
+                "kv_cache_config.num_blocks")
+
+        # Use the first layer as representative for `num_blocks` in shape calc.
+        first_layer_name = kv_cache_group.layer_names[0]
+        rep_tensor_cfg = layer_to_tensor_cfg[first_layer_name]
+        num_blocks = rep_tensor_cfg.size // kv_cache_spec.page_size_bytes
+
+        # Attention backend for this group is the first one initialised in
+        # `initialize_attn_backend()` which must have been called already.
+        attn_backend_cls = self.attn_backends[0]
+        kv_cache_shape = attn_backend_cls.get_kv_cache_shape(
+            num_blocks,
+            kv_cache_spec.block_size,
+            kv_cache_spec.num_kv_heads,
+            kv_cache_spec.head_size,
+        )
+
+        num_layers = len(kv_cache_group.layer_names)
+        dtype = kv_cache_spec.dtype
+        kv_cache_buffers = self.kvcached_interfaces.alloc_kv_cache(
+            kv_cache_shape,
+            kv_cache_spec.block_size,
+            dtype,
+            self.device.type,
+            num_layers,
+            attention_type="MHA",
+            kv_layout="NHD",
+        )
+
+        # Create layer_name -> tensor mapping from raw buffers.
+
+        kv_cache_group = kv_cache_config.kv_cache_groups[0]
+
+        kv_caches: dict[str, torch.Tensor] = {}
+        for idx, layer_name in enumerate(kv_cache_group.layer_names):
+            kv_caches[layer_name] = kv_cache_buffers[idx]
+
+        return kv_caches
