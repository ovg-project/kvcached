diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py
index 74f3f7852..3f402d16b 100644
--- a/vllm/v1/core/block_pool.py
+++ b/vllm/v1/core/block_pool.py
@@ -279,3 +279,91 @@ class BlockPool:
             The KV cache usage (between 0.0 and 1.0).
         """
         return 1.0 - (self.get_num_free_blocks() / self.num_gpu_blocks)
+
+
+class ElasticBlockPool:
+    """ElasticBlockPool that manages KVCacheBlocks.
+    It provides same interface as BlockPool, but it leverages kvcached for
+    elastic KV cachememory management.
+    """
+
+    def __init__(self, num_gpu_blocks: int, block_size: int, cell_size: int,
+                 num_layers: int, enable_caching: bool):
+        assert isinstance(num_gpu_blocks, int) and num_gpu_blocks > 0
+        assert not enable_caching, "Caching is not supported in ElasticBlockPool"
+        self.num_gpu_blocks = num_gpu_blocks
+
+        from kvcached.slab_allocator import KVCacheManager  # type: ignore
+        self.kv_cache_manager = KVCacheManager(num_gpu_blocks, block_size,
+                                               cell_size, num_layers)
+
+    def get_cached_block(self,
+                         block_hash: BlockHashType) -> Optional[KVCacheBlock]:
+        return False
+
+    def cache_full_blocks(
+        self,
+        request: Request,
+        blocks: list[KVCacheBlock],
+        block_hashes: list[BlockHashType],
+        num_cached_blocks: int,
+        num_full_blocks: int,
+        block_size: int,
+        hash_fn: Callable,
+    ) -> None:
+        raise NotImplementedError(
+            "Caching is not supported in ElasticBlockPool")
+
+    def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:
+        """Get new blocks from the free block pool.
+
+        Note that we do not check block cache in this function.
+
+        Args:
+            num_blocks: The number of blocks to allocate.
+
+        Returns:
+            A list of new block.
+        """
+        if num_blocks > self.get_num_free_blocks():
+            raise ValueError(
+                f"Cannot get {num_blocks} free blocks from the pool")
+
+        block_ids = self.kv_cache_manager.alloc(num_blocks)
+        assert len(block_ids) == num_blocks
+
+        return [KVCacheBlock(bid) for bid in block_ids]
+
+    def touch(self, blocks: list[KVCacheBlock]) -> None:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -> None:
+        """Free a list of blocks. The blocks should be ordered by their
+        eviction priority, where the first block will be evicted first.
+
+        Args:
+            ordered_blocks: A list of blocks to free ordered by their eviction
+                priority.
+        """
+        block_ids = [block.block_id for block in ordered_blocks]
+        if len(block_ids) > 0:
+            self.kv_cache_manager.free(block_ids)
+
+    def reset_prefix_cache(self) -> bool:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def get_num_free_blocks(self) -> int:
+        """Get the number of free blocks in the pool.
+
+        Returns:
+            The number of free blocks.
+        """
+        return self.kv_cache_manager.available_size()
+
+    def get_usage(self) -> float:
+        """Get the KV cache usage.
+
+        Returns:
+            The KV cache usage (between 0.0 and 1.0).
+        """
+        return 1.0 - (self.get_num_free_blocks() / self.num_gpu_blocks)
diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 33761cf7f..bc6f0fd84 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -55,7 +55,24 @@ class KVCacheManager:
         self.num_preallocate_blocks = cdiv(num_preallocate_tokens,
                                            self.block_size)
 
-        self.block_pool = BlockPool(self.num_gpu_blocks, enable_caching)
+        import os
+        self.enable_kvcached = os.getenv("ENABLE_KVCACHED",
+                                         "false").lower() == "true"
+        if self.enable_kvcached:
+            from vllm.v1.core.block_pool import (
+                ElasticBlockPool)  # type: ignore
+            if self.enable_caching:
+                raise ValueError("Caching is not supported for kvcached")
+            # cell_size is the size of the k/v cache tensor for a single token.
+            cell_size = kv_cache_spec.page_size_bytes // self.block_size // 2
+            self.block_pool = ElasticBlockPool(self.num_gpu_blocks,
+                                               self.block_size,
+                                               cell_size=cell_size,
+                                               num_layers=len(
+                                                   kv_cache_config.tensors),
+                                               enable_caching=enable_caching)
+        else:
+            self.block_pool = BlockPool(self.num_gpu_blocks, enable_caching)
 
         self.specialized_manager = get_specialized_manager(
             kv_cache_spec=kv_cache_spec,
@@ -176,7 +193,7 @@ class KVCacheManager:
             new_computed_blocks: A list of new computed blocks just hitting the
                 prefix caching.
             num_lookahead_tokens: The number of speculative tokens to allocate.
-                This is used by spec decode proposers with kv-cache such 
+                This is used by spec decode proposers with kv-cache such
                 as eagle.
 
         Blocks layout:
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 70e8bd75e..794465606 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -272,6 +272,14 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                                         pin_memory=self.pin_memory)
         self.seq_lens_np = self.seq_lens_cpu.numpy()
 
+        import os
+        self.enable_kvcached = os.getenv("ENABLE_KVCACHED",
+                                         "false").lower() == "true"
+        if self.enable_kvcached:
+            import kvcached.ops as kvcached_ops
+            kvcached_ops.init_kvcached()
+            self.kvcached_ops = kvcached_ops
+
     def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
         """Update the cached states and the persistent batch with the scheduler
         output.
@@ -1640,6 +1648,45 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
         kv_caches: dict[str, torch.Tensor] = {}
 
+        if self.enable_kvcached:
+            for kv_cache_group in kv_cache_config.kv_cache_groups:
+                kv_cache_spec = kv_cache_group.kv_cache_spec
+                for layer_name in kv_cache_group.layer_names:
+                    if not isinstance(kv_cache_spec, FullAttentionSpec):
+                        raise ValueError(
+                            "kvcached only supports full attention")
+                    tensor_config = kv_cache_config.tensors[layer_name]
+                    assert tensor_config.size % kv_cache_spec.page_size_bytes == 0
+                    num_blocks = tensor_config.size // kv_cache_spec.page_size_bytes
+                    assert num_blocks >= kv_cache_config.num_blocks
+
+            num_layers = len(kv_cache_config.tensors)
+            layer_name = list(kv_cache_config.tensors.keys())[0]
+            kv_cache_spec = kv_cache_config.kv_cache_groups[0].kv_cache_spec
+            tensor_config = kv_cache_config.tensors[layer_name]
+
+            dtype = kv_cache_spec.dtype
+            num_blocks = tensor_config.size // kv_cache_spec.page_size_bytes
+            assert num_blocks >= kv_cache_config.num_blocks
+            kv_cache_shape = self.attn_backend.get_kv_cache_shape(
+                num_blocks, kv_cache_spec.block_size,
+                kv_cache_spec.num_kv_heads, kv_cache_spec.head_size)
+
+            kv_cache_buffers = self.kvcached_ops.vllm_alloc_kv_cache(
+                kv_cache_shape, kv_cache_spec.block_size, dtype,
+                self.device.type, num_layers)
+            layer_id = 0
+            for kv_cache_group in kv_cache_config.kv_cache_groups:
+                for layer_name in kv_cache_group.layer_names:
+                    kv_caches[layer_name] = kv_cache_buffers[layer_id]
+                    layer_id += 1
+
+            bind_kv_cache(
+                kv_caches,
+                self.vllm_config.compilation_config.static_forward_context,
+                self.kv_caches)
+            return
+
         for kv_cache_group in kv_cache_config.kv_cache_groups:
             kv_cache_spec = kv_cache_group.kv_cache_spec
             for layer_name in kv_cache_group.layer_names:
