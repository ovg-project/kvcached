diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 635536a97..8ae8c03a9 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -1407,7 +1407,9 @@ class Scheduler(
             if not self.enable_hierarchical_cache
             else self.max_total_num_tokens - protected_size
         )
-        if memory_leak:
+        # disable memory leak check for elastic allocator, since the available size changes dynamically.
+        enable_elastic_memory = get_bool_env_var("ENABLE_KVCACHED", "false")
+        if memory_leak and not enable_elastic_memory:
             msg = (
                 "token_to_kv_pool_allocator memory leak detected! "
                 f"{available_size=}, {protected_size=}, {self.max_total_num_tokens=}\n"
diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py
index 6d06fa103..e4820b981 100644
--- a/python/sglang/srt/mem_cache/allocator.py
+++ b/python/sglang/srt/mem_cache/allocator.py
@@ -153,6 +153,43 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         return self._kvcache.load_cpu_copy(kv_cache_cpu, indices)
 
 
+class ElasticTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
+    def __init__(self, size: int, dtype: torch.dtype, device: str, kvcache: KVCache):
+        super().__init__(size, 1, dtype, device, kvcache)
+        self.clear()
+
+        # sanity check
+        from sglang.srt.mem_cache.memory_pool import ElasticMHATokenToKVPool
+
+        if not isinstance(kvcache, ElasticMHATokenToKVPool):
+            raise ValueError(
+                f"ElasticTokenToKVPoolAllocator requires kvcache to be an ElasticMHATokenToKVPool, but got {type(kvcache)}"
+            )
+
+        self.kvcached_allocator = kvcache.kvcached_allocator
+
+        if "cuda" not in device:
+            raise ValueError("ElasticTokenToKVPoolAllocator only supports cuda device")
+
+    def available_size(self):
+        return self.kvcached_allocator.available_size()
+
+    def alloc(self, need_size: int):
+        indices: list[int] = self.kvcached_allocator.alloc(need_size)
+        indices = torch.tensor(indices, dtype=torch.int32, device="cuda")
+        return indices
+
+    def free(self, free_index: torch.Tensor):
+        if self.is_not_in_free_group:
+            indices: list[int] = free_index.cpu().numpy().tolist()
+            return self.kvcached_allocator.free(indices)
+        else:
+            self.free_group.append(free_index)
+
+    def clear(self):
+        if hasattr(self, "kvcached_allocator"):
+            self.kvcached_allocator.clear()
+
 class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
     """Allocator for SWA hybrid KV cache."""
 
diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py
index 00ad66552..c35b4a955 100644
--- a/python/sglang/srt/mem_cache/memory_pool.py
+++ b/python/sglang/srt/mem_cache/memory_pool.py
@@ -438,6 +438,88 @@ class MHATokenToKVPool(KVCache):
         )
 
 
+class ElasticMHATokenToKVPool(MHATokenToKVPool):
+    def __init__(
+        self,
+        size: int,
+        page_size: int,
+        dtype: torch.dtype,
+        head_num: int,
+        head_dim: int,
+        layer_num: int,
+        device: str,
+        enable_memory_saver: bool,
+        start_layer: Optional[int] = None,
+        end_layer: Optional[int] = None,
+        enable_overlap_schedule: bool = True,
+    ):
+        super(MHATokenToKVPool, self).__init__(
+            size=size,
+            page_size=page_size,
+            dtype=dtype,
+            layer_num=layer_num,
+            device=device,
+            enable_memory_saver=enable_memory_saver,
+            start_layer=start_layer,
+            end_layer=end_layer,
+        )
+        self.head_num = head_num
+        self.head_dim = head_dim
+        self.custom_mem_pool = None
+
+        try:
+            import kvcached.integration.sglang.interfaces as kvcached_interfaces
+
+            self.kvcached_interfaces = kvcached_interfaces
+            self.kvcached_interfaces.init_kvcached(async_sched=enable_overlap_schedule)
+
+            # Initialize KV allocator based on per-token KV size (cell_size)
+            self.cell_size = self.head_num * self.head_dim * self.dtype.itemsize
+
+            self.kvcached_allocator = kvcached_interfaces.get_kv_cache_manager(
+                self.size + self.page_size,
+                self.page_size,
+                self.cell_size,
+                num_layers=layer_num,
+            )
+        except ImportError as e:
+            raise ImportError(
+                "kvcached is not found. Please install kvcached with `pip install kvcached --no-build-isolation` to use elastic KV cache."
+            ) from e
+
+        self._create_buffers()
+
+        self.layer_transfer_counter = None
+        self.device_module = torch.get_device_module(self.device)
+        self.alt_stream = self.device_module.Stream() if _is_cuda else None
+
+        k_size, v_size = self.get_kv_size_bytes()
+        logger.info(
+            f"KV Cache is allocated. #tokens: {size}, K size: {k_size / GB:.2f} GB, V size: {v_size / GB:.2f} GB"
+        )
+        self.mem_usage = (k_size + v_size) / GB
+
+    def __del__(self):
+        self.kvcached_interfaces.shutdown_kvcached()
+        del self.kvcached_allocator
+        self.k_buffer = None
+        self.v_buffer = None
+
+    def _create_buffers(self):
+        if "cuda" not in self.device:
+            raise ValueError("ElasticMHATokenToKVPool only supports cuda device")
+
+        self.k_buffer, self.v_buffer = self.kvcached_interfaces.alloc_kv_cache(
+            kvcache_shape=(self.size + self.page_size, self.head_num, self.head_dim),
+            dtype=self.dtype,
+            device=self.device,
+            num_layers=self.layer_num,
+            page_size=self.page_size,
+            attention_type="MHA",
+            kv_layout="NHD",
+        )
+
+
 class SWAKVPool(KVCache):
     """KV cache with separate pools for full and SWA attention layers."""
 
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 051f2b75e..742cce652 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -75,6 +75,7 @@ from sglang.srt.managers.schedule_batch import (
 from sglang.srt.mem_cache.allocator import (
     AscendPagedTokenToKVPoolAllocator,
     BaseTokenToKVPoolAllocator,
+    ElasticTokenToKVPoolAllocator,
     PagedTokenToKVPoolAllocator,
     SWATokenToKVPoolAllocator,
     TokenToKVPoolAllocator,
@@ -83,6 +84,7 @@ from sglang.srt.mem_cache.memory_pool import (
     AscendMLAPagedTokenToKVPool,
     AscendTokenToKVPool,
     DoubleSparseTokenToKVPool,
+    ElasticMHATokenToKVPool,
     MHATokenToKVPool,
     MLATokenToKVPool,
     ReqToTokenPool,
@@ -194,6 +196,8 @@ class ModelRunner:
         self.req_to_token_pool = req_to_token_pool
         self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
         self.is_hybrid = model_config.is_hybrid
+        # Use environment variable to enable elastic memory
+        self.is_elastic = get_bool_env_var("ENABLE_KVCACHED", "false")
         self.use_mla_backend = self.model_config.attention_arch == AttentionArch.MLA
         self.attention_chunk_size = model_config.attention_chunk_size
 
@@ -1182,8 +1186,8 @@ class ModelRunner:
                     device=self.device,
                 )
             else:
-                self.token_to_kv_pool = MHATokenToKVPool(
-                    self.max_total_num_tokens,
+                mha_token_to_kv_pool_args = dict(
+                    size=self.max_total_num_tokens,
                     page_size=self.page_size,
                     dtype=self.kv_cache_dtype,
                     head_num=self.model_config.get_num_kv_heads(
@@ -1196,6 +1200,19 @@ class ModelRunner:
                     start_layer=self.start_layer,
                     end_layer=self.end_layer,
                 )
+                if not self.is_elastic:
+                    self.token_to_kv_pool = MHATokenToKVPool(
+                        **mha_token_to_kv_pool_args
+                    )
+                else:
+                    # add enable overlap schedule args to elastic mha token to kv pool
+                    elastic_mha_token_to_kv_pool_args = mha_token_to_kv_pool_args
+                    elastic_mha_token_to_kv_pool_args["enable_overlap_schedule"] = (
+                        not self.server_args.disable_overlap_schedule
+                    )
+                    self.token_to_kv_pool = ElasticMHATokenToKVPool(
+                        **elastic_mha_token_to_kv_pool_args
+                    )
 
         if self.token_to_kv_pool_allocator is None:
             if self.page_size == 1:
@@ -1208,7 +1225,12 @@ class ModelRunner:
                         kvcache=self.token_to_kv_pool,
                     )
                 else:
-                    self.token_to_kv_pool_allocator = TokenToKVPoolAllocator(
+                    allocator_cls = (
+                        ElasticTokenToKVPoolAllocator
+                        if self.is_elastic
+                        else TokenToKVPoolAllocator
+                    )
+                    self.token_to_kv_pool_allocator = allocator_cls(
                         self.max_total_num_tokens,
                         dtype=self.kv_cache_dtype,
                         device=self.device,
